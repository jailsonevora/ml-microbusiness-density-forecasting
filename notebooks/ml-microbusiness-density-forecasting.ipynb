{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":41881,"databundleVersionId":4781186,"sourceType":"competition"}],"dockerImageVersionId":30357,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-01-12T10:12:07.115845Z","iopub.execute_input":"2023-01-12T10:12:07.116935Z","iopub.status.idle":"2023-01-12T10:12:07.149706Z","shell.execute_reply.started":"2023-01-12T10:12:07.116820Z","shell.execute_reply":"2023-01-12T10:12:07.148720Z"},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/godaddy-microbusiness-density-forecasting/sample_submission.csv\n/kaggle/input/godaddy-microbusiness-density-forecasting/census_starter.csv\n/kaggle/input/godaddy-microbusiness-density-forecasting/train.csv\n/kaggle/input/godaddy-microbusiness-density-forecasting/test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Import library","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Deal W. Missing Values\nfrom sklearn.impute import SimpleImputer #Imputation technic\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Deal W. Categorical Variables\nfrom sklearn.preprocessing import OrdinalEncoder #Ordinal Encoder technic\nfrom sklearn.preprocessing import OneHotEncoder #One Hot Encoder technic\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom scipy import stats\nfrom sklearn.compose import make_column_selector, make_column_transformer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.cluster import KMeans\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.utils.validation import check_array, check_is_fitted\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.decomposition import PCA\nfrom category_encoders import MEstimateEncoder\nfrom sklearn.preprocessing import RobustScaler\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom tqdm.notebook import tqdm# import the tqdm library for progress display\n\n# store the path of the input directory in the BASE variable\nBASE = '../input/godaddy-microbusiness-density-forecasting/'","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# define the smape function which calculates and prints the SMAPE \n# (Symmetric Mean Absolute Percentage Error) of the predicted values y_pred and the \n# true values y_true\ndef smape(y_true, y_pred):\n    smap = np.zeros(len(y_true))\n    \n    num = np.abs(y_true - y_pred)\n    dem = ((np.abs(y_true) + np.abs(y_pred)) / 2)\n    \n    pos_ind = (y_true!=0)|(y_pred!=0)\n    smap[pos_ind] = num[pos_ind] / dem[pos_ind]\n    \n    return 100 * np.mean(smap)\n\n# define the vsmape function which calculates and returns the element-wise SMAPE values of y_true and y_pred in an array\ndef vsmape(y_true, y_pred):\n    smap = np.zeros(len(y_true))\n    \n    num = np.abs(y_true - y_pred)\n    dem = ((np.abs(y_true) + np.abs(y_pred)) / 2)\n    \n    pos_ind = (y_true!=0)|(y_pred!=0)\n    smap[pos_ind] = num[pos_ind] / dem[pos_ind]\n    \n    return 100 * smap\n\n\n!ls ../input/godaddy-microbusiness-density-forecasting","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load data","metadata":{}},{"cell_type":"code","source":"census = pd.read_csv(BASE + 'census_starter.csv')\nprint(census.columns)\ncensus.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-12T10:13:13.636624Z","iopub.execute_input":"2023-01-12T10:13:13.637078Z","iopub.status.idle":"2023-01-12T10:13:13.939999Z","shell.execute_reply.started":"2023-01-12T10:13:13.637028Z","shell.execute_reply":"2023-01-12T10:13:13.938599Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"train = pd.read_csv(BASE + 'train.csv')\ntest = pd.read_csv(BASE + 'test.csv')\nsub = pd.read_csv(BASE + 'sample_submission.csv')\nprint(train.shape, test.shape, sub.shape)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Concat train and test","metadata":{}},{"cell_type":"code","source":"train['istest'] = 0\ntest['istest'] = 1\nraw = pd.concat((train, test)).sort_values(['cfips','row_id']).reset_index(drop=True)\n\n\nraw['first_day_of_month'] = pd.to_datetime(raw[\"first_day_of_month\"])\nraw['county'] = raw.groupby('cfips')['county'].ffill()\nraw['state'] = raw.groupby('cfips')['state'].ffill()\nraw[\"year\"] = raw[\"first_day_of_month\"].dt.year\nraw[\"month\"] = raw[\"first_day_of_month\"].dt.month\nraw[\"dcount\"] = raw.groupby(['cfips'])['row_id'].cumcount()\nraw['county_i'] = (raw['county'] + raw['state']).factorize()[0]\nraw['state_i'] = raw['state'].factorize()[0]\nraw.tail(20)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-12T10:15:40.863971Z","iopub.execute_input":"2023-01-12T10:15:40.864578Z","iopub.status.idle":"2023-01-12T10:15:40.887979Z","shell.execute_reply.started":"2023-01-12T10:15:40.864527Z","shell.execute_reply":"2023-01-12T10:15:40.886100Z"},"trusted":true},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                 cfips          county    state first_day_of_month  \\\nrow_id                                                               \n1001_2019-08-01   1001  Autauga County  Alabama         2019-08-01   \n1001_2019-09-01   1001  Autauga County  Alabama         2019-09-01   \n1001_2019-10-01   1001  Autauga County  Alabama         2019-10-01   \n1001_2019-11-01   1001  Autauga County  Alabama         2019-11-01   \n1001_2019-12-01   1001  Autauga County  Alabama         2019-12-01   \n\n                 microbusiness_density  active  \nrow_id                                          \n1001_2019-08-01               3.007682    1249  \n1001_2019-09-01               2.884870    1198  \n1001_2019-10-01               3.055843    1269  \n1001_2019-11-01               2.993233    1243  \n1001_2019-12-01               2.993233    1243  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cfips</th>\n      <th>county</th>\n      <th>state</th>\n      <th>first_day_of_month</th>\n      <th>microbusiness_density</th>\n      <th>active</th>\n    </tr>\n    <tr>\n      <th>row_id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1001_2019-08-01</th>\n      <td>1001</td>\n      <td>Autauga County</td>\n      <td>Alabama</td>\n      <td>2019-08-01</td>\n      <td>3.007682</td>\n      <td>1249</td>\n    </tr>\n    <tr>\n      <th>1001_2019-09-01</th>\n      <td>1001</td>\n      <td>Autauga County</td>\n      <td>Alabama</td>\n      <td>2019-09-01</td>\n      <td>2.884870</td>\n      <td>1198</td>\n    </tr>\n    <tr>\n      <th>1001_2019-10-01</th>\n      <td>1001</td>\n      <td>Autauga County</td>\n      <td>Alabama</td>\n      <td>2019-10-01</td>\n      <td>3.055843</td>\n      <td>1269</td>\n    </tr>\n    <tr>\n      <th>1001_2019-11-01</th>\n      <td>1001</td>\n      <td>Autauga County</td>\n      <td>Alabama</td>\n      <td>2019-11-01</td>\n      <td>2.993233</td>\n      <td>1243</td>\n    </tr>\n    <tr>\n      <th>1001_2019-12-01</th>\n      <td>1001</td>\n      <td>Autauga County</td>\n      <td>Alabama</td>\n      <td>2019-12-01</td>\n      <td>2.993233</td>\n      <td>1243</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-12T10:15:50.453184Z","iopub.execute_input":"2023-01-12T10:15:50.454568Z","iopub.status.idle":"2023-01-12T10:15:50.467181Z","shell.execute_reply.started":"2023-01-12T10:15:50.454516Z","shell.execute_reply":"2023-01-12T10:15:50.465746Z"},"trusted":true},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                 cfips first_day_of_month\nrow_id                                   \n1001_2022-11-01   1001         2022-11-01\n1003_2022-11-01   1003         2022-11-01\n1005_2022-11-01   1005         2022-11-01\n1007_2022-11-01   1007         2022-11-01\n1009_2022-11-01   1009         2022-11-01","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cfips</th>\n      <th>first_day_of_month</th>\n    </tr>\n    <tr>\n      <th>row_id</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1001_2022-11-01</th>\n      <td>1001</td>\n      <td>2022-11-01</td>\n    </tr>\n    <tr>\n      <th>1003_2022-11-01</th>\n      <td>1003</td>\n      <td>2022-11-01</td>\n    </tr>\n    <tr>\n      <th>1005_2022-11-01</th>\n      <td>1005</td>\n      <td>2022-11-01</td>\n    </tr>\n    <tr>\n      <th>1007_2022-11-01</th>\n      <td>1007</td>\n      <td>2022-11-01</td>\n    </tr>\n    <tr>\n      <th>1009_2022-11-01</th>\n      <td>1009</td>\n      <td>2022-11-01</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"# Clean and transform","metadata":{}},{"cell_type":"markdown","source":"There are some anomalies, specially at timestep 18","metadata":{}},{"cell_type":"code","source":"lag = 1\nraw[f'mbd_lag_{lag}'] = raw.groupby('cfips')['microbusiness_density'].shift(lag).bfill()\nraw['dif'] = (raw['microbusiness_density'] / raw[f'mbd_lag_{lag}']).fillna(1).clip(0, None) - 1\nraw.loc[(raw[f'mbd_lag_{lag}']==0), 'dif'] = 0\nraw.loc[(raw[f'microbusiness_density']>0) & (raw[f'mbd_lag_{lag}']==0), 'dif'] = 1\nraw['dif'] = raw['dif'].abs()\nraw.groupby('dcount')['dif'].sum().plot()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Outliers","metadata":{}},{"cell_type":"code","source":"outliers = []\ncnt = 0\nfor o in tqdm(raw.cfips.unique()):\n    indices = (raw['cfips']==o)\n    tmp = raw.loc[indices].copy().reset_index(drop=True)\n    var = tmp.microbusiness_density.values.copy()\n    #vmax = np.max(var[:38]) - np.min(var[:38])\n    \n    for i in range(37, 2, -1):\n        thr = 0.20*np.mean(var[:i])\n        difa = abs(var[i]-var[i-1])\n        if (difa>=thr):\n            var[:i] *= (var[i]/var[i-1])\n            outliers.append(o)\n            cnt+=1\n    var[0] = var[1]*0.99\n    raw.loc[indices, 'microbusiness_density'] = var\n    \noutliers = np.unique(outliers)\nlen(outliers), cnt","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lag = 1\nraw[f'mbd_lag_{lag}'] = raw.groupby('cfips')['microbusiness_density'].shift(lag).bfill()\nraw['dif'] = (raw['microbusiness_density'] / raw[f'mbd_lag_{lag}']).fillna(1).clip(0, None) - 1\nraw.loc[(raw[f'mbd_lag_{lag}']==0), 'dif'] = 0\nraw.loc[(raw[f'microbusiness_density']>0) & (raw[f'mbd_lag_{lag}']==0), 'dif'] = 1\nraw['dif'] = raw['dif'].abs()\nraw.groupby('dcount')['dif'].sum().plot()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"raw.loc[raw.cfips == 1013].plot(x='dcount', y='microbusiness_density')\nraw.loc[raw.cfips == 21215].plot(x='dcount', y='microbusiness_density')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Convert target","metadata":{}},{"cell_type":"code","source":"raw['target'] = raw.groupby('cfips')['microbusiness_density'].shift(-1)\nraw['target'] = raw['target']/raw['microbusiness_density'] - 1\n\nraw.loc[raw['cfips']==28055, 'target'] = 0.0\nraw.loc[raw['cfips']==48269, 'target'] = 0.0\n\nraw.iloc[-20:,:20]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"raw['target'].clip(-0.05, 0.05).hist(bins=100)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"raw['lastactive'] = raw.groupby('cfips')['active'].transform('last')\n\ndt = raw.loc[raw.dcount==28].groupby('cfips')['microbusiness_density'].agg('last')\nraw['lasttarget'] = raw['cfips'].map(dt)\n\nraw['lastactive'].clip(0, 8000).hist(bins=30)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Features Engineering","metadata":{}},{"cell_type":"code","source":"def build_features(raw, target='microbusiness_density', target_act='active_tmp', lags = 6):\n    feats = []\n    for lag in range(1, lags):\n        raw[f'mbd_lag_{lag}'] = raw.groupby('cfips')[target].shift(lag)\n        raw[f'act_lag_{lag}'] = raw.groupby('cfips')[target_act].diff(lag)\n        feats.append(f'mbd_lag_{lag}')\n        feats.append(f'act_lag_{lag}')\n        \n    lag = 1\n    for window in [2, 4, 6]:\n        raw[f'mbd_rollmea{window}_{lag}'] = raw.groupby('cfips')[f'mbd_lag_{lag}'].transform(lambda s: s.rolling(window, min_periods=1).sum())        \n        #raw[f'mbd_rollmea{window}_{lag}'] = raw[f'mbd_lag_{lag}'] - raw[f'mbd_rollmea{window}_{lag}']\n        feats.append(f'mbd_rollmea{window}_{lag}')\n        \n    return raw, feats","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Build Features based in lag of target\nraw, feats = build_features(raw, 'target', 'active', lags = 4)\nfeatures = ['state_i']\nfeatures += feats\nprint(features)\nraw.loc[raw.dcount==38, features].head(10)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"raw['lasttarget'].clip(0,10).hist(bins=100)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"blacklist = [\n    'North Dakota', 'Iowa', 'Kansas', 'Nebraska', 'South Dakota','New Mexico', 'Alaska', 'Vermont'\n]\nblacklistcfips = [\n1019,1027,1029,1035,1039,1045,1049,1057,1067,1071,1077,1085,1091,1099,1101,1123,1131,1133,4001,4012,4013,4021,4023,5001,5003,5005,5017,5019,5027,5031,5035,5047,5063,5065,5071,5081,5083,5087,5091,5093,5107,5109,5115,5121,5137,5139,5141,5147,6003,6015,6027,6033,6053,6055,6057,6071,6093,6097,6103,6105,6115,8003,8007,8009,8019,8021,8023,8047,8051,8053,8055,8057,8059,8061,8065,8067,8069,8071,8073,8075,8085,8091,8093,8097,8099,8103,8105,8107,8109,8111,8115,8117,8121,9007,9009,9015,12009,12017,12019,12029,12047,12055,12065,12075,12093,12107,12127,13005,13007,13015,13017,13019,13027,13035,13047,13065,13081,13083,13099,13107,13109,13117,13119,13121,13123,13125,13127,13135,13143,13147,13161,13165,13171,13175,13181,13193,13201,13221,13225,13229,13231,13233,13245,13247,13249,13257,13279,13281,13287,13289,13293,13301,13319,15001,15005,15007,16001,16003,16005,16007,16013,16015,16017,16023,16025,16029,16031,16033,16035,16037,16043,16045,16049,16061,16063,16067,17001,17003,17007,17009,17013,17015,17023,17025,17031,17035,17045,17051,17059,17061,17063,17065,17067,17069,17075,17077,17081,17085,17087,17103,17105,17107,17109,17115,17117,17123,17127,17133,17137,17141,17143,17147,17153,17167,17169,17171,17177,17179,17181,17185,17187,17193,18001,18007,18009,18013,18015,18019,18021,18025,18035,18037,18039,18041,18053,18061,18075,18079,18083,18087,18099,18103,18111,18113,18115,18137,18139,18145,18153,18171,18179,21001,21003,21013,21017,21023,21029,21035,21037,21039,21045,21047,21055,21059,21065,21075,21077,21085,21091,21093,21097,21099,21101,21103,21115,21125,21137,21139,21141,21149,21155,21157,21161,21165,21179,21183,21191,21197,21199,21215,21217,21223,21227,21237,21239,22019,22021,22031,22039,22041,22047,22069,22085,22089,22101,22103,22109,22111,22115,22119,22121,23003,23009,23021,23027,23029,24011,24027,24029,24031,24035,24037,24039,24041,25011,25015,26003,26007,26011,26019,26021,26025,26027,26033,26037,26041,26043,26051,26053,26057,26059,26061,26065,26071,26077,26079,26083,26089,26097,26101,26103,26109,26111,26115,26117,26119,26127,26129,26131,26135,26141,26143,26155,26161,26165,27005,27011,27013,27015,27017,27021,27023,27025,27029,27047,27051,27055,27057,27065,27069,27073,27075,27077,27079,27087,27091,27095,27101,27103,27105,27107,27109,27113,27117,27119,27123,27125,27129,27131,27133,27135,27141,27147,27149,27155,27159,27167,27169,28017,28019,28023,28025,28035,28045,28049,28061,28063,28093,28097,28099,28125,28137,28139,28147,28159,29001,29015,29019,29031,29033,29041,29049,29051,29055,29057,29063,29065,29069,29075,29085,29089,29101,29103,29111,29121,29123,29125,29135,29137,29139,29143,29157,29159,29161,29167,29171,29173,29175,29177,29183,29195,29197,29199,29203,29205,29207,29209,29213,29215,29217,29223,29227,29229,30005,30009,30025,30027,30033,30035,30037,30039,30045,30049,30051,30053,30055,30057,30059,30069,30071,30073,30077,30079,30083,30085,30089,30091,30093,30101,30103,30105,30107,30109,32005,32009,32017,32023,32027,32029,32510,33005,33007,34021,34027,34033,34035,36011,36017,36023,36033,36043,36047,36049,36051,36057,36061,36067,36083,36091,36097,36103,36107,36113,36115,36121,36123,37005,37009,37011,37017,37023,37029,37031,37049,37061,37075,37095,37117,37123,37131,37137,37151,37187,37189,37197,39005,39009,39015,39017,39019,39023,39037,39039,39043,39049,39053,39057,39063,39067,39071,39077,39085,39087,39091,39097,39105,39107,39113,39117,39119,39125,39127,39129,39135,39137,39151,39153,39157,40003,40013,40015,40023,40025,40027,40035,40039,40043,40045,40053,40055,40057,40059,40065,40067,40073,40077,40079,40099,40105,40107,40111,40115,40123,40127,40129,40133,40141,40147,40151,40153,41001,41007,41013,41015,41017,41021,41025,41031,41033,41037,41051,41055,41063,41067,41069,42005,42007,42011,42013,42015,42019,42027,42029,42031,42035,42053,42057,42067,42071,42083,42085,42093,42097,42105,42111,42113,42115,42123,42125,42127,42129,44005,44007,44009,45001,45009,45021,45025,45031,45059,45067,45071,45073,45089,47001,47005,47013,47015,47019,47021,47023,47027,47035,47039,47041,47047,47055,47057,47059,47061,47069,47073,47075,47077,47083,47087,47099,47105,47121,47127,47131,47133,47135,47137,47147,47151,47153,47159,47161,47163,47169,47177,47183,47185,48001,48011,48017,48019,48045,48057,48059,48063,48065,48073,48077,48079,48081,48083,48087,48095,48101,48103,48107,48109,48115,48117,48119,48123,48125,48129,48149,48151,48153,48155,48159,48161,48165,48175,48189,48191,48195,48197,48211,48221,48229,48233,48235,48237,48239,48241,48243,48245,48255,48261,48263,48265,48267,48269,48275,48277,48283,48293,48299,48305,48311,48313,48319,48321,48323,48327,48333,48345,48347,48355,48369,48377,48379,48383,48387,48389,48401,48403,48413,48417,48431,48433,48437,48443,48447,48453,48455,48457,48461,48463,48465,48469,48471,48481,48483,48485,48487,48495,48499,49001,49009,49013,49019,49027,49031,49045,51005,51017,51025,51029,51031,51036,51037,51043,51057,51059,51065,51071,51073,51077,51079,51083,51091,51095,51097,51101,51111,51115,51119,51121,51127,51135,51147,51155,51159,51165,51167,51171,51173,51181,51183,51191,51197,51530,51590,51610,51620,51670,51678,51720,51735,51750,51770,51810,51820,53013,53019,53023,53031,53033,53037,53039,53041,53047,53065,53069,53071,53075,54013,54019,54025,54031,54033,54041,54049,54055,54057,54063,54067,54071,54077,54079,54085,54089,54103,55001,55003,55005,55007,55011,55017,55021,55025,55029,55037,55043,55047,55049,55051,55061,55065,55067,55075,55077,55091,55097,55101,55103,55109,55117,55123,55125,55127,56007,56009,56011,56015,56017,56019,56021,56027,56031,56037,56043,56045,\n12061,  6095, 49025, 18073, 29029, 29097, 48419, 51830, 30067, 26095, 18159, 32001, 54065, 54027, 13043, 48177, 55069, 48137, 30087, 29007, 13055, 48295, 28157, 29037, 45061, 22053, 13199, 47171, 53001, 55041, 51195, 18127, 29151, 48307, 51009, 16047, 29133,  5145, 17175, 21027, 48357, 29179, 13023, 16077, 48371, 21057, 16039, 21143, 48435, 48317, 48475,  5129, 36041, 48075, 29017, 47175, 39167, 47109, 17189, 17173, 28009, 39027, 48133, 18129, 48217, 40081, 36021,  6005, 42099, 18051, 36055, 53051, 6109, 21073, 27019,  6051, 48055,  8083, 48503, 17021, 10003, 41061, 22001, 22011, 21205, 48223, 51103, 51047, 16069, 17033, 41011,  6035, 47145, 27083, 18165, 36055, 12001, 26159,  8125, 34017,\n28141, 55119, 48405, 40029, 18125, 21135, 29073, 55115, 37149,55039, 26029, 12099, 13251, 48421, 39007, 41043, 22015, 37115,54099, 51137, 22049, 55131, 17159, 56001, 40005, 18017, 28091,47101, 27037, 29005, 13239, 21019, 55085, 48253, 51139, 40101,13283, 18049, 39163, 45049, 51113,\n]\nACT_THR = 1.8\nABS_THR = 1.00\nraw['ypred_last'] = np.nan\nraw['ypred'] = np.nan\nraw['k'] = 1.\nVAL = []\nBEST_ROUNDS = []\nfor TS in range(29, 38):\n    print(TS)\n    \n    model = xgb.XGBRegressor(\n        objective='reg:pseudohubererror',\n        #objective='reg:squarederror',\n        tree_method=\"hist\",\n        n_estimators=4999,\n        learning_rate=0.0075,\n        max_leaves = 17,\n        subsample=0.50,\n        colsample_bytree=0.50,\n        max_bin=4096,\n        n_jobs=2,\n        eval_metric='mae',\n        early_stopping_rounds=70,\n    )\n            \n    train_indices = (raw.istest==0) & (raw.dcount  < TS) & (raw.dcount >= 1) & (raw.lastactive>ACT_THR)  & (raw.lasttarget>ABS_THR) \n    valid_indices = (raw.istest==0) & (raw.dcount == TS)\n    model.fit(\n        raw.loc[train_indices, features],\n        raw.loc[train_indices, 'target'].clip(-0.0043, 0.0045),\n        eval_set=[(raw.loc[valid_indices, features], raw.loc[valid_indices, 'target'])],\n        verbose=500,\n    )\n    best_rounds = model.best_iteration\n    BEST_ROUNDS.append(model.best_iteration)\n    ypred = model.predict(raw.loc[valid_indices, features])\n    raw.loc[valid_indices, 'k'] = ypred + 1\n    raw.loc[valid_indices,'k'] = raw.loc[valid_indices,'k'] * raw.loc[valid_indices,'microbusiness_density']\n\n    # Validate\n    lastval = raw.loc[raw.dcount==TS, ['cfips', 'microbusiness_density']].set_index('cfips').to_dict()['microbusiness_density']\n    dt = raw.loc[raw.dcount==TS, ['cfips', 'k']].set_index('cfips').to_dict()['k']\n    \n    df = raw.loc[raw.dcount==(TS+1), ['cfips', 'microbusiness_density', 'state', 'lastactive', 'mbd_lag_1']].reset_index(drop=True)\n    df['pred'] = df['cfips'].map(dt)\n    df['lastval'] = df['cfips'].map(lastval)\n    \n    df.loc[df['lastactive']<=ACT_THR, 'pred'] = df.loc[df['lastactive']<=ACT_THR, 'lastval']\n    df.loc[df['lastval']<=ABS_THR, 'pred'] = df.loc[df['lastval']<=ABS_THR, 'lastval']\n    df.loc[df['state'].isin(blacklist), 'pred'] = df.loc[df['state'].isin(blacklist), 'lastval']\n    df.loc[df['cfips'].isin(blacklistcfips), 'pred'] = df.loc[df['cfips'].isin(blacklistcfips), 'lastval']\n    raw.loc[raw.dcount==(TS+1), 'ypred'] = df['pred'].values\n    raw.loc[raw.dcount==(TS+1), 'ypred_last'] = df['lastval'].values\n    \n    print(f'TS: {TS}')\n    print('Last Value SMAPE:', smape(df['microbusiness_density'], df['lastval']) )\n    print('XGB SMAPE:', smape(df['microbusiness_density'], df['pred']))\n    print()\n\n\nind = (raw.dcount>=30)&(raw.dcount<=38)\nprint( 'XGB SMAPE:', smape( raw.loc[ind, 'microbusiness_density'],  raw.loc[ind, 'ypred'] ) )\nprint( 'Last Value SMAPE:', smape( raw.loc[ind, 'microbusiness_density'],  raw.loc[ind, 'ypred_last'] ) )","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"raw['error'] = vsmape(raw['microbusiness_density'], raw['ypred'])\nraw['error_last'] = vsmape(raw['microbusiness_density'], raw['ypred_last'])\nraw.loc[(raw.dcount==30), ['microbusiness_density', 'ypred', 'error', 'error_last'] ]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dt = raw.loc[(raw.dcount>=30)&(raw.dcount<=38) ].groupby(['cfips','dcount'])['error', 'error_last'].last()\ndt['miss'] = dt['error'] > dt['error_last']\ndt = dt.groupby('cfips')['miss'].mean()\ndt = dt.loc[dt>=0.50]\ndt.shape","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(dt.index)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"','.join([str(i) for i in dt.index])","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for d in dt.index[:10]:\n    raw.loc[raw.cfips==d].plot(x='dcount', y=['microbusiness_density', 'ypred'], title=str(d))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"raw.iloc[-40:, :16]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.mean( BEST_ROUNDS ), np.median( BEST_ROUNDS ), BEST_ROUNDS","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_rounds = int(np.median( BEST_ROUNDS )+1)\nbest_rounds","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"TS = 38\nprint(TS)\n\nmodel0 = xgb.XGBRegressor(\n    objective='reg:pseudohubererror',\n    #objective='reg:squarederror',\n    tree_method=\"hist\",\n    n_estimators=best_rounds,\n    learning_rate=0.0075,\n    max_leaves = 31,\n    subsample=0.60,\n    colsample_bytree=0.50,\n    max_bin=4096,\n    n_jobs=2,\n    eval_metric='mae',\n)\nmodel1 = xgb.XGBRegressor(\n    objective='reg:pseudohubererror',\n    #objective='reg:squarederror',\n    tree_method=\"hist\",\n    n_estimators=best_rounds,\n    learning_rate=0.0075,\n    max_leaves = 31,\n    subsample=0.60,\n    colsample_bytree=0.50,\n    max_bin=4096,\n    n_jobs=2,\n    eval_metric='mae',\n)\n\ntrain_indices = (raw.istest==0) & (raw.dcount  < TS) & (raw.dcount >= 1) & (raw.lastactive>ACT_THR)  & (raw.lasttarget>ABS_THR) \nvalid_indices = (raw.dcount == TS)\nmodel0.fit(\n    raw.loc[train_indices, features],\n    raw.loc[train_indices, 'target'].clip(-0.0044, 0.0046),\n)\nmodel1.fit(\n    raw.loc[train_indices, features],\n    raw.loc[train_indices, 'target'].clip(-0.0044, 0.0046),\n)\n\nypred = (model0.predict(raw.loc[valid_indices, features]) + model1.predict(raw.loc[valid_indices, features]))/2\nraw.loc[valid_indices, 'k'] = ypred + 1.\nraw.loc[valid_indices,'k'] = raw.loc[valid_indices,'k'] * raw.loc[valid_indices,'microbusiness_density']\n\n# Validate\nlastval = raw.loc[raw.dcount==TS, ['cfips', 'microbusiness_density']].set_index('cfips').to_dict()['microbusiness_density']\ndt = raw.loc[raw.dcount==TS, ['cfips', 'k']].set_index('cfips').to_dict()['k']","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = raw.loc[raw.dcount==(TS+1), ['cfips', 'microbusiness_density', 'state', 'lastactive', 'mbd_lag_1']].reset_index(drop=True)\ndf['pred'] = df['cfips'].map(dt)\ndf['lastval'] = df['cfips'].map(lastval)\n\ndf.loc[df['lastactive']<=ACT_THR, 'pred'] = df.loc[df['lastactive']<=ACT_THR, 'lastval']\ndf.loc[df['lastval']<=ABS_THR, 'pred'] = df.loc[df['lastval']<=ABS_THR, 'lastval']\ndf.loc[df['state'].isin(blacklist), 'pred'] = df.loc[df['state'].isin(blacklist), 'lastval']\ndf.loc[df['cfips'].isin(blacklistcfips), 'pred'] = df.loc[df['cfips'].isin(blacklistcfips), 'lastval']\nraw.loc[raw.dcount==(TS+1), 'ypred'] = df['pred'].values\nraw.loc[raw.dcount==(TS+1), 'ypred_last'] = df['lastval'].values","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"raw[['cfips','microbusiness_density','dcount','ypred','ypred_last','k']].tail(20)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"raw.loc[raw['cfips']==28055, 'microbusiness_density'] = 0\nraw.loc[raw['cfips']==48269, 'microbusiness_density'] = 1.762115\n\ndt = raw.loc[raw.dcount==39, ['cfips', 'ypred']].set_index('cfips').to_dict()['ypred']\ntest = raw.loc[raw.istest==1, ['row_id', 'cfips','microbusiness_density']].copy()\ntest['microbusiness_density'] = test['cfips'].map(dt)\n\ntest[['row_id','microbusiness_density']].to_csv('submission.csv', index=False)\ntest.head(40)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Define model","metadata":{}},{"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(x, y, test_size=0.2, random_state=2020)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"[X_train.shape, y_train.shape]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"[X_valid.shape, y_valid.shape]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\ncategorical_cols = [cname for cname in X_train.columns if\n                    #X_train[cname].nunique() < 10 and \n                    X_train[cname].dtype == \"object\"]\n\n# Select numerical columns\nnumerical_cols = [cname for cname in X_train.columns if \n                X_train[cname].dtype in ['int64', 'float64']]\n\n# Preprocessing for numerical data\nnumerical_transformer = Pipeline(steps=[\n       ('imputer', SimpleImputer(strategy='constant'))\n      ,('standardize', StandardScaler())\n])\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n        ('imputer', SimpleImputer(strategy='constant')),\n        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n        #('encoder', OrdinalEncoder())\n]) \n\n# Bundle preprocessing for numerical and categorical data\n# Instead of using a transformer, you can specify the string \"drop\"\n# if you want the columns to be dropped, or you can specify\n# \"passthrough\" if you want the columns to be left untouched. By\n# default, the remaining columns (i.e., the ones that were not listed)\n# will be dropped, but you can set the remainder hyperparameter to\n# any transformer (or to \"passthrough\") if you want these columns\n# to be handled differently.\npreprocessor = ColumnTransformer(\n                transformers=[\n                    #('robustScaler', RobustScaler()),\n                    #('reduce_dims', PCA(n_components=4)),\n                    ('num', numerical_transformer, numerical_cols),\n                    ('cat', categorical_transformer, categorical_cols)\n                ])","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Fine-Tune model","metadata":{}},{"cell_type":"markdown","source":"## XGBoost","metadata":{}},{"cell_type":"code","source":"xgb_dflt = XGBRegressor()\n\nparam_lst_xgb = {\n    'learning_rate' : [0.01, 0.03, 0.1, 0.15, 0.3, 0.5],\n    'n_estimators' : [100, 500, 1000, 2000, 3000, 4000, 7000, 10000, 15000],\n    'max_depth' : [3, 6, 9],\n    'min_child_weight' : [1, 5, 10, 20],\n    'reg_alpha' : [0.001, 0.01, 0.1],\n    'reg_lambda' : [0.001, 0.01, 0.1]\n}\n\nxgb_reg = RandomizedSearchCV(\n                                estimator = xgb_dflt, \n                                param_distributions = param_lst_xgb,\n                                n_iter = 100, \n                                scoring = 'neg_mean_absolute_error',\n                                n_jobs = 20,\n                                cv = 5\n                            )\n\n# Bundle preprocessing and modeling code in a pipeline\nxgb_pipeline = Pipeline(steps=[\n                                ('preprocessor', preprocessor),\n                                ('model_xgb_RandomizedSearchCV', xgb_reg)\n                             ])\n       \nxgb_search = xgb_pipeline.fit(X_train, y_train)\n\n# XGB with tune hyperparameters\nxgb = XGBRegressor(**xgb_search.named_steps['model_xgb_RandomizedSearchCV'].best_params_)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Bundle preprocessing and modeling code in a pipeline\nxgb_pipeline_best_params = Pipeline(steps=[\n                                ('preprocessor', preprocessor),\n                                ('model', xgb)\n                             ])\n\nxgb_pipeline_best_params.fit(X_train, y_train)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"xgb_search.named_steps['model_xgb_RandomizedSearchCV'].best_params_","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"xgb_search.named_steps['model_xgb_RandomizedSearchCV'].best_score_","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"xgb_search.named_steps['model_xgb_RandomizedSearchCV'].best_estimator_","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#xgb_search.named_steps['model_xgb'].cv_results_\nxgb_search.named_steps['model_xgb_RandomizedSearchCV'].cv_results_['mean_test_score']","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def plot_maxdeph_score(rd_search):\n#     max_depth = [item['model__max_depth'] for item in rd_search.cv_results_['params']]\n#     scores = list(rd_search.cv_results_['mean_test_score'])\n#     d = pd.DataFrame([max_depth, scores]).T\n#     d.columns = ['Max Depth','Score']\n#     d.groupby(['Max Depth']).mean()\n#     return d","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# plot_maxdeph_score(xgb_search)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## LightGBM","metadata":{}},{"cell_type":"code","source":"lgbm_dflt = LGBMRegressor(boosting_type='gbdt',objective='regression', max_depth=-1,\n                    lambda_l1=0.0001, lambda_l2=0, learning_rate=0.1,\n                    n_estimators=100, max_bin=200, min_child_samples=20, \n                    bagging_fraction=0.75, bagging_freq=5,\n                    bagging_seed=7, feature_fraction=0.8,\n                    feature_fraction_seed=7, verbose_eval=False)\n\nparam_lst_lgbm = {\n    'max_depth' : [2, 5, 8, 10],\n    'learning_rate' : [0.001, 0.01, 0.03, 0.1, 0.2],\n    'n_estimators' : [100, 300, 500, 1000, 1500, 4000, 7000, 10000, 15000],\n    'lambda_l1' : [0.0001, 0.001, 0.01],\n    'lambda_l2' : [0, 0.0001, 0.001, 0.01],\n    'feature_fraction' : [0.4, 0.6, 0.8],\n    'min_child_samples' : [5, 10, 20, 25]\n}\n\nlightgbm = RandomizedSearchCV(estimator = lgbm_dflt, \n                              param_distributions = param_lst_lgbm,\n                              n_iter = 100, \n                              scoring = 'neg_mean_absolute_error',\n                              n_jobs = 20,\n                              cv = 5)\n\nlgbm_pipeline = Pipeline(steps=[\n                                ('preprocessor', preprocessor),\n                                ('model_lightgbm_RandomizedSearchCV', lightgbm)\n                             ])\n\nlightgbm_search = lgbm_pipeline.fit(X_train, y_train)\n\n# LightBGM with tuned hyperparameters\nlgbm = LGBMRegressor(**lightgbm_search.named_steps['model_lightgbm_RandomizedSearchCV'].best_params_, verbose_eval=False)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lgbm_pipeline_best_params = Pipeline(steps=[\n                                ('preprocessor', preprocessor),\n                                ('model', lgbm)\n                             ])\n\nlgbm_pipeline_best_params.fit(X_train, y_train)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lightgbm_search.named_steps['model_lightgbm_RandomizedSearchCV'].best_score_","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## CatBoost","metadata":{}},{"cell_type":"code","source":"cb_dflt = CatBoostRegressor(logging_level='Silent')\n\nparam_lst_cb = {\n    'n_estimators' : [100, 300, 500, 1000, 1300, 1600, 4000, 7000, 10000, 15000],\n    'learning_rate' : [0.0001, 0.001, 0.01, 0.03, 0.1],\n    'l2_leaf_reg' : [0.001, 0.01, 0.1],\n    'random_strength' : [0.25, 0.5 ,1],\n    'max_depth' : [3, 6, 9],\n    'min_child_samples' : [2, 5, 10, 15, 20],\n    'rsm' : [0.5, 0.7, 0.9]\n}\n\ncatboost = RandomizedSearchCV(estimator = cb_dflt, \n                              param_distributions = param_lst_cb,\n                              n_iter = 100, \n                              scoring = 'neg_mean_absolute_error', \n                              n_jobs = 20,\n                              cv = 5)\n\ncb_pipeline = Pipeline(steps=[\n                                ('preprocessor', preprocessor),\n                                ('model_catboost_RandomizedSearchCV', catboost)\n                             ])\n\ncatboost_search = cb_pipeline.fit(X_train, y_train)\n\n# CatBoost with tuned hyperparams\ncb = CatBoostRegressor(**catboost_search.named_steps['model_catboost_RandomizedSearchCV'].best_params_, logging_level='Silent')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Bundle preprocessing and modeling code in a pipeline\ncb_pipeline = Pipeline(steps=[\n                                ('preprocessor', preprocessor),\n                                ('model', cb)\n                             ])\n\ncb_pipeline.fit(X_train, y_train)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"catboost_search.named_steps['model_catboost_RandomizedSearchCV'].best_score_","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"markdown","source":"## Cross validation trainning","metadata":{}},{"cell_type":"code","source":"# cros_rmses = -cross_val_score(my_pipeline, X_train, y_train, scoring=\"neg_mean_absolute_error\", cv=5)\n\n\ndef mean_cross_val(model, X, y):\n    score = cross_val_score(model, X, y,scoring=\"neg_mean_absolute_error\", cv=5)\n    mean = score.mean()\n    return mean","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Validate best pipe with test data","metadata":{}},{"cell_type":"code","source":"preds = xgb_pipeline.predict(X_valid)\n\nscore_mae = mean_absolute_error(y_valid, preds)\nrmse_xgb = np.sqrt(mean_squared_error(y_valid, preds))\nscore_xgb = xgb_pipeline.score(X_valid, y_valid)\n#cv_xgb = mean_cross_val(xgb_pipeline, x, y)\n\nprint('MAE:', score_mae)\nprint('Score:', score_xgb)\nprint('RMSE:', rmse_xgb)\n#print('CV:', cv_xgb)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preds = lgbm_pipeline.predict(X_valid)\n\nscore_mae = mean_absolute_error(y_valid, preds)\nrmse_xgb = np.sqrt(mean_squared_error(y_valid, preds))\nscore_lgbm = lgbm_pipeline.score(X_valid, y_valid)\n#cv_lgbm = mean_cross_val(lgbm_pipeline, x, y)\n\nprint('MAE:', score_mae)\nprint('Score:', score_lgbm)\nprint('RMSE:', rmse_xgb)\n#print('CV:', cv_lgbm)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preds = cb_pipeline.predict(X_valid)\n\nscore_mae = mean_absolute_error(y_valid, preds)\nrmse_xgb = np.sqrt(mean_squared_error(y_valid, preds))\nscore_cb = cb_pipeline.score(X_valid, y_valid)\n#cv_cb = mean_cross_val(cb_pipeline, x, y)\n\nprint('MAE:', score_mae)\nprint('Score:', score_cb)\nprint('RMSE:', rmse_xgb)\n#print('CV:', cv_cb)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Test evaluation","metadata":{}},{"cell_type":"code","source":" def blend_models_predict(X, b, c, d):\n        return ((b* xgb_pipeline.predict(X)) + (c * lgbm_pipeline.predict(X)) + (d * cb_pipeline.predict(X)))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"subm = np.exp(blend_models_predict(test, 0.4, 0.3, 0.3))\nprint(subm)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Save test predictions to file","metadata":{}}]}